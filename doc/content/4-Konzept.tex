 
\chapter{Konzept}
\label{sec:konzept}

\section{Vorgehensweise}
\label{sec:konzept:vorgehensweise}
Für die Umsetzung in diesem Projekt eigen sich der Apriori Algorithmus (oder eine Variante davon), welcher im ersten Schritt des \nameref{sec:recherche:dataminingtechniken:disziplinen:association} (\cref{sec:recherche:dataminingtechniken:disziplinen:association}) verwendet wird sowie die \nameref{sec:recherche:dataminingtechniken:disziplinen:clusteranalysis} (\cref{sec:recherche:dataminingtechniken:disziplinen:clusteranalysis}), welche in diesem Abschnitt weiter beschrieben werden.

\nameref{sec:recherche:dataminingtechniken:disziplinen:classification} (\cref{sec:recherche:dataminingtechniken:disziplinen:classification}) fällt weg, da damit bekannte Klassen zugeteilt werden welche in diesem Projekt jedoch nicht bekannt sind. Die \nameref{sec:recherche:dataminingtechniken:disziplinen:regression} (\cref{sec:recherche:dataminingtechniken:disziplinen:regression}) eignet sich nicht, da dadurch nur numerische Werte vorausgesagt werden können, in dieser Arbeit jedoch Klassen vergeben werden müssen. \nameref{sec:recherche:dataminingtechniken:disziplinen:collaborativefiltering} (\cref{sec:recherche:dataminingtechniken:disziplinen:collaborativefiltering}) versucht durch Kundenbewertungen ähnliche Objekte vorzuschlagen. Dies kann für einen Recommender eingesetzt werden, jedoch nicht für die Auffindung von häufigen Attributen.

%Die Umsetzung in diesem Projekt wird zweistufig durchgeführt. Als erstes gibt der User eine Abfrage ein, für welche anschliessend eine eine Liste von häufig auftretenden Attributen gesucht werden soll (siehe \cref{sec:einletung:ziel} \nameref{sec:einletung:ziel}). Die Abfrage schränkt dabei den Datenbestand ein. Dafür kann der erste Schritt des \nameref{sec:recherche:dataminingtechniken:disziplinen:association} (\cref{sec:recherche:dataminingtechniken:disziplinen:association}) eingesetzt werden. Für die Analyse der Restmenge eignet sich die \nameref{sec:recherche:dataminingtechniken:disziplinen:clusteranalysis} (\cref{sec:recherche:dataminingtechniken:disziplinen:clusteranalysis}), da zu Beginn die Klassen noch nicht bekannt sind. Dadurch fällt \nameref{sec:recherche:dataminingtechniken:disziplinen:classification} (\cref{sec:recherche:dataminingtechniken:disziplinen:classification}) weg. Die \nameref{sec:recherche:dataminingtechniken:disziplinen:regression} (\cref{sec:recherche:dataminingtechniken:disziplinen:regression}) eignet sich nicht, da dadurch nur numerische Werte vorausgesagt werden können, in dieser Arbeit jedoch Klassen vergeben werden müssen. \nameref{sec:recherche:dataminingtechniken:disziplinen:collaborativefiltering} (\cref{sec:recherche:dataminingtechniken:disziplinen:collaborativefiltering}) versucht durch Kundenbewertungen ähnliche Objekte vorzuschlagen. Dies kann für einen Recommender eingesetzt werden, jedoch nicht für die Auffindung von häufigen Attributen.

% Assoziationsanalyse
%Als erstes werden Attributkombinationen (Mengen von Attributen) in der Datenbasis gesucht, welche häufig auftauchen. Dies wird durch den A-priori Algorithmus (oder Abwandlungen dadurch) erreicht. Zu Beginn wird eine mindestens Prozentzahl definiert, wie oft ein Attributmenge auftauchen muss, der sogenannte Minimal-Support. Anschliessend werden für alle 1-elementigen Mengen überpüft, ob sie den . Danach werden diese um ein Attribut erweitert und es wird erneut gezählt.  Wird dieser Wert nicht erreicht, wird die Menge als uninteressant eingestuft und nicht weiter verfolgt. 

\subsection{Einschränkung des Datenbestandes}
Im ersten Schritt wird der Datenbestand durch die Auswahl von Attributen durch den Benutzer eingeschränkt. 

%Dafür eignet sich die Häufigkeitszählung des \nameref{sec:recherche:dataminingtechniken:disziplinen:association} (\cref{sec:recherche:dataminingtechniken:disziplinen:association}), welche in diesem Abschnitt genauer beschrieben wird.

\subsection{Apriori Algorithmus}
Der Apriori Algorithmus ist der erste von zwei Schritten des \nameref{sec:recherche:dataminingtechniken:disziplinen:association} (\cref{sec:recherche:dataminingtechniken:disziplinen:association}), dessen Ziel es ist häufige Attribut-Kombinationen aufzufinden. 

Als erstes wird ein Mindestsupport $supmin$ definiert, welches Aussagt, wie oft eine Attributmenge $t$ in einer Instanz $I$ in der Datenmenge $D$ vorkommen muss, damit diese als relevant betrachtet werden kann. Als erstes werden alle einzelnen Attribute (1-Attributmenge) gezählt und diejenigen gestrichen, welche $submin$ nicht erfüllen.

Danach wird Iterativ die Attributmenge um ein Element erweitert und und wiederum auf $submin$ überprüft, bis keine Elemente mehr den Mindestsupport erreichen.

\begin{enumerate}
\item Für jedes $t_k$ mit $|t_k| = k$ wird gesucht wie oft es gebucht worden ist.
	\begin{center}
	$cnt(t_k) = \big|\{I \in D | t_k \subseteq I \}\big|$
	\end{center}
	Die Mengen werden in der Liste $C_k$ zusammengefasst.
\item Für jedes Attribut wird der Support $sup$ ausgerechnet.
	\begin{center}
	$sup(t_k) = \cfrac{cnt(t_k)}{|D|}$
	\end{center}
\item Es werden alle Attribute entfernt, welche $submin$ nicht erfüllen.
	\begin{center}
	$sup(t_k) \ge supmin$
	\end{center}
	Die übrig gebliebenen Mengen werden in $L_k$ vereint.
\item \todo{sort}
\item Es wird $L_k$ mit Elementen aus $C_k$ erweitert und ergibt $C_{k+1}$
	\begin{center}
	$ C_{k+1} = \{l_1 \cup l_2 | \forall l_1 \in L_k, \forall l_2 \in L_k \} $
	\end{center}
\item Entferne Elemente aus $C_{k+1}$, welche nicht in $L_k$ enthalten sind.
\item Wenn $C_{k+1} \neq \emptyset$ dann $k = k+1$ und gehe zu Schritt 1. 
\item Wenn $C_{k+1} = \emptyset$ dann breche ab und gib $\bigcup\limits_{i=1}^{k} L_{i}$ zurück.
\end{enumerate}

In der \cref{fig:konzept:vorgehensweise:apriorialgorithmus} wird der Ablauf noch anhand eines Beispieles veranschaulicht.

\begin{figure}[H]
	\RawFloats
	\centering
	\includegraphics[width=0.8\textwidth]{images/Apriori-Algorithmus.png}
	\caption{Visualisierung des Apriori Algorithmus}
	\source{\cite{association_rule_learning_2017-01-05}}
	\label{fig:konzept:vorgehensweise:apriorialgorithmus}
\end{figure}

$D$ wird benötigt um die Vorkommen zu zählen. Die roten Elemente in $L$ werden entfernt weil $minsup$ nicht erreicht wurde, welches im Beispiel den Wert 2 hat. Die Elemente in $C$ können gelöscht werden, da sich der Apriori Algorithmus die Eigenschaft zunutze macht, dass wenn in $L_{k-1}$ ein Element $minsup$ nicht erfüllt, kann es dies auch nicht in $C_k$\todo{cite}.

Der Algorithmus kann somit in drei Schritte aufgeteilt werden.
\begin{enumerate}
\item Häufigkeitszählung
\item Generierung der x+1-Attributmenge (Apriori-gen Funktion, Join step)
\item Entfernung der Elemente welche den $minsup$ nicht erfüllen (Apriori-gen Funktion, Prune step)
\end{enumerate}

Die nachfolgende Apriori-Methode führt die Häufigkeitszählung durch.
\begin{lstlisting}
procedure apriori($D$: Datenmenge; $minsup$: Minimum support) {
	$L_1 \gets$ find_frequent_1-itemsets($D$)
	$k \gets 2$
	while $L_{k-1} \neq \emptyset$ {
		$C_k \gets$ apriori_gen($L_{k-1}$)
		foreach transaction $I \in D$ {
			$C_I \gets \{ c | c \in C_k \wedge c \subseteq I \}$
			foreach candidate $c \in C_I$ {
				count[$c$] $\gets$ count[$c$] + 1
			}
		}
		$L_k \gets \{c | c \in C_k \wedge$ count[$c$] $\ge minsup \}$
		$k \gets k + 1$
	}
	return $\bigcup\limits_{k} L_{k}$
}
\end{lstlisting}
Übergeben wird die gesamte Datenmenge $D$, welche für die Häufigkeitszählung verwendet wird, sowie der minimum Support welche die Elemente zu erfüllen haben.
Im ersten Schritt wird die 1-Attributmenge mit häufigen Elementen definiert. In den Zeilen 3 bis 12 wird für $k > 2$ die Items gesammelt, welche öfters vorkommen als der $minsup$. Dazu wird mittels der apriori\_gen Funktion die \textit{Candidates} $C_k$ generiert.
Anschliessend werden die vorkommen von allen Elementen in $D$ gezählt, wie oft sie in $C_k$ vorkommen.


\begin{lstlisting}
procedure apriori_gen($L_{k-1}$:frequent (k-1)-itemsets) {
	$C_k \gets \emptyset$
	foreach itemset $l_1 \in L_{k-1}$ {
		foreach itemset $l_2 \in L_{k-1}$ {
			$c \gets \{l_1 \cup \{i\} | i \in l_2 \wedge l_1 \cup \{i\} \in C_k \}$
			$c_{pruned} \gets c - \{ i | i \subset c \wedge |i| = k-1 \wedge i \notin L_{k-1}) \}$
			$C_k \gets C_k \cup c_{pruned}$
		}
	}
	return $C_k$
}
\end{lstlisting}
In der apriori\_gen Funktion werden k-Datenmengen generiert, indem eine k-1 Datenmenge erweitert wird. Dazu gibt es zwei Schleifen welche durchlaufen werden. Auf Zeile 5 wird $l_1$ um Elemente von $l_2$ erweitert. Zudem wird sichergestellt, dass gleiche Mengen nicht mehrfach hinzugefügt werden.

Auf Zeile 6 werden die neuen

% Longer version of apriori_gen
%$c \gets \{l_1 \cup \{i\} | i \in l_2 \wedge l_1 \cup \{i\} \in C_k \}$
%$c_{pruned} \gets \{ i | i \in c \wedge \forall j(j \subset i \wedge |j| = k-1 \wedge j \in L_{k-1}) \}$
%$C_k \gets C_k \cup c_{pruned}$
%
% Looped version of apriori_gen
%	$C_k \gets \emptyset$
%	foreach itemset $l_1 \in L_{k-1}$ {
%		foreach itemset $l_2 \in L_{k-1}$ {
%			if (($l_1$[1] = $l_2$[1]) $\wedge$ ($l_1$[2] = $l_2$[2]) $\wedge$ ... $\wedge$ ($l_1$[$k$-2] = $l_2$[$k$-2]) $\wedge$ ($l_1$[$k$-1] $<$ $l_2$[$k$-1])) {
%				$c \gets l_1 \cup l_2$
%				if not has_infrequent_subset($c, L_{k-1}$) {
%					delete $c$
%				} else {
%					$C_k \gets C_k \cup c$
%					add $C_k \cup c$ 
%				}
%			}
%		}
%	}
%	return $C_k$
%
%
%\begin{lstlisting}
%procedure has_infrequent_subset($c$: candidate k-itemset; $L_{k-1}$: frequent (k-1)-itemsets) {
%	foreach (k-1)-subset $s of c$ {
%		if $s \notin L_{k-1}$ {
%			return TRUE
%		}
%	}
%	return FALSE
%}
%\end{lstlisting}